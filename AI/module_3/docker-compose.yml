services:
  embeddings:
    build:
      context: .
      dockerfile: Dockerfile.embeddings
    ports:
      - 8080:8080
    volumes:
      - ../models:/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 600s
      timeout: 5s
      retries: 5
      # Allows for initial model downloads
      # In a production environment, you likely want to pre-cache the embedding model
      start_period: 100000s
      start_interval: 5s

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - 6333:6333
      - 6334:6334
    volumes:
      - qdrant_storage:/qdrant/storage
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c ':> /dev/tcp/127.0.0.1/6333' || exit 1
      interval: 600s
      timeout: 5s
      retries: 5
      start_period: 100000s
      start_interval: 5s

  localai:
    image: localai/localai:v2.25.0-ffmpeg-core
    ports:
      - 8081:8080
    environment:
      - LOG_LEVEL=INFO
      - MODELS_PATH=/models
    volumes:
      - ../models:/models:cached
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 600s
      timeout: 5s
      retries: 5
      # Allows for initial model downloads
      # In a production environment, you likely want to pre-cache the AI model
      start_period: 100000s
      start_interval: 5s

  pdfqa:
    build:
      context: .
      dockerfile: Dockerfile.pdfqa
    ports:
      - 9000:9000
    environment:
      - EMBEDDINGS_SERVICE_URL=http://embeddings:8080
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - LOCALAI_URL=http://localai:8080/v1
    depends_on:
      embeddings:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      localai:
        condition: service_healthy

volumes:
  qdrant_storage:
